{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input shape :  4D Tensor (batch , heigth , width , channeLs)\n",
    "#Output shape: 5D Tensor (batch , height , width , n_boxes , 8)\n",
    "\n",
    "#Последнияя ось содержит 4 координаты bbox - (cx,cy,w,h) + 4 значения вариации для каждой коробки\n",
    "\n",
    "class AnchorBoxes:\n",
    "    \n",
    "    def __init__(self,img_height=300,\n",
    "                 img_width=300,\n",
    "                 this_scale=scales[0],\n",
    "                 next_scale=scales[1],\n",
    "                 aspect_ratios=[0.5,1.0,2.0],\n",
    "                 two_boxes_for_ar1=True,\n",
    "                 this_steps=None,\n",
    "                 this_offsets=None,\n",
    "                 clip_boxes=False,\n",
    "                 variances=[0.1,0.1,0.2,0.2],\n",
    "                 coords='centroids',\n",
    "                 n_boxes=4,#!!!!!!!!!!!!!!!! Убрать это должно рассчитыаваться автоматически после __init__\n",
    "                 normalize_coords=False):\n",
    "    \n",
    "        self.img_width=img_width\n",
    "        self.img_height=img_height\n",
    "        self.this_scale=scales[0]\n",
    "        self.next_scale=scales[1]\n",
    "        self.aspect_ratios=aspect_ratios\n",
    "        self.two_boxes_for_ar1=two_boxes_for_ar1\n",
    "        self.this_steps=this_steps\n",
    "        self.this_offsets=this_offsets\n",
    "        self.clip_boxes=clip_boxes\n",
    "        self.variances=variances\n",
    "        self.coords=coords\n",
    "        self.normalize_coords=normalize_coords\n",
    "        self.n_boxes=n_boxes#!!!!!!!!!!!!!!!!!! Убрать это должно рассчитыаваться автоматически после __init__\n",
    "    \n",
    "    #Рассчитаем w и h для каждой bbox ,используя scales & aspect ratio\n",
    "    \n",
    "    def call(self,Layer):\n",
    "        \n",
    "        size=min(self.img_height,self.img_width)\n",
    "        \n",
    "        wh_list=[] #место для хранения w и h каждой из рассчитанных bbox - ов\n",
    "        \n",
    "        #вычисляем w & h для каждого aspect ratio\n",
    "        for ar in self.aspect_ratios:\n",
    "            \n",
    "            if (ar == 1):\n",
    "                \n",
    "                #Рассчитываем обычные w & h для aspect ratio 1\n",
    "                box_height=box_width=self.this_scale*size\n",
    "                \n",
    "                wh_list.append((box_width,box_height))\n",
    "                \n",
    "                if self.two_boxes_for_ar1:\n",
    "                    \n",
    "                    #Вычисляем немного более увеличенные версии w & h , используя sqrt , this_scale & next_scale\n",
    "                    box_height=box_width=np.sqrt(self.this_scale*self.next_scale)*size\n",
    "                    \n",
    "                    wh_list.append((box_width,box_height))\n",
    "                    \n",
    "            else:\n",
    "                    \n",
    "                box_height=self.this_scale*size/np.sqrt(ar) #formula1 for height\n",
    "\n",
    "                box_width=self.this_scale*np.sqrt(ar)*size #formula2 for width\n",
    "\n",
    "                wh_list.append((box_width,box_height))\n",
    "                    \n",
    "            wh_list=np.array(wh_list)\n",
    "            \n",
    "            #we need to get the shape of the Input Tensor\n",
    "            \n",
    "            #HERE NUMPY WE NEED TO REPLACE TO TENSORFLOW .get_shape()\n",
    "            batch_size,feature_map_height,feature_map_width,feature_map_chennles=Layer.shape\n",
    "            #HERE NUMPY WE NEED TO REPLACE TO TENSORFLOW .get_shape() \n",
    "            \n",
    "            # Compute the grid of box center points. They are identical for all aspect ratios.\n",
    "            \n",
    "            # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
    "            \n",
    "            if (self.this_steps is None):\n",
    "                \n",
    "                step_height=self.img_height / feature_map_height\n",
    "                \n",
    "                step_width=self.img_width / feature_map_width\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                if isinstance(self.this_steps,(list,tuple)) and (len(self.this_steps)==2):\n",
    "                    \n",
    "                    step_height=self.this_step[0]\n",
    "                    \n",
    "                    step_width=self.this_steps[1]\n",
    "                    \n",
    "                elif isinstance(self.this_steps,(int,float)):\n",
    "                    \n",
    "                    step_height=self.this_steps\n",
    "                    \n",
    "                    step_width=self.this_steps\n",
    "                    \n",
    "            # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.         \n",
    "                    \n",
    "            if (self.this_offsets is None):\n",
    "                \n",
    "                offset_height=0.5\n",
    "                \n",
    "                offset_width=0.5\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                if isinstance(self.this_offsets,(list,tuple)) and (len(self.this_offsets)==2):\n",
    "                    \n",
    "                    offset_height=self.this_offsets[0]\n",
    "                    \n",
    "                    offset_width=self.this_offsets[1]\n",
    "                    \n",
    "                elif isinstance(self.this_steps,(int,float)):\n",
    "                    \n",
    "                    step_height=self.this_steps\n",
    "                    \n",
    "                    step_width=self.this_steps\n",
    "                    \n",
    "            \n",
    "            # Now that we have the offsets and step sizes, compute the grid of anchor box center points.  \n",
    "            \n",
    "            cy=np.linspace(offset_height*step_height,(offset_height+feature_map_height -1)*step_height,feature_map_height)\n",
    "            \n",
    "            cx=np.linspace(offset_width*step_width,(offset_width+feature_map_width-1)*step_width,feature_map_width)\n",
    "            \n",
    "            cx_grid,cy_grid=np.meshgrid(cx,cy)\n",
    "            \n",
    "            cx_grid=np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "            cy_grid=np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "            \n",
    "            # Create a 4D tensor template of shape (feature_map_height, feature_map_width, n_boxes, 4)\n",
    "            # where the last dimension will contain (cx, cy, w, h) \n",
    "            \n",
    "            \n",
    "            \n",
    "            boxes_tensor=np.zeros((feature_map_height,feature_map_width,self.n_boxes,4))\n",
    "            \n",
    "            #НЕОБХОДИМО В НАЧАЛЕ УСТАНОВИТЬ ЗНАЧЕНИЕ ДЛЯ n_boxes\n",
    "        \n",
    "            boxes_tensor[:,:,:,0]=np.tile(cx_grid,(1,1,self.n_boxes)) #Set cx\n",
    "            boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
    "            boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
    "            boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
    "            \n",
    "            \n",
    "            # Convert (cx, cy, w, h) to (xmin, xmax, ymin, ymax)\n",
    "            \n",
    "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners') \n",
    "            \n",
    "            # If clip_boxes is enabled, clip the coordinates to lie within the image boundaries\n",
    "            \n",
    "            if self.clip_boxes:\n",
    "                \n",
    "                x_coords=boxes_tensor[:,:,:,[0,2]]\n",
    "                x_coords[x_coords >= self.imd_width]=self.img_width-1\n",
    "                x_coords[x_coords<0]=0\n",
    "                boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
    "                y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
    "                y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
    "                y_coords[y_coords < 0] = 0\n",
    "                boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
    "                \n",
    "            # If normalize_coords is enabled, normalize the coordinates to be within [0,1]\n",
    "            \n",
    "            if self.normalize_coords:\n",
    "                \n",
    "                boxes_tensor[:,:,:,[0,2]] /=self.img_width\n",
    "                boxes_tensor[:, :, :, [1, 3]] /= self.img_heigh\n",
    "                \n",
    "            # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
    "            if self.coords == 'centroids':\n",
    "                # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
    "                boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')\n",
    "            elif self.coords == 'minmax':\n",
    "                # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
    "                boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')\n",
    "                \n",
    "          \n",
    "            # Create a tensor to contain the variances and append it to boxes_tensor. This tensor has the same shape\n",
    "            # as boxes_tensor and simply contains the same 4 variance values for every position in the last axis.\n",
    "            \n",
    "            variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)\n",
    "            variances_tensor += self.variances\n",
    "            \n",
    "            \n",
    "            # Now boxes_tensor becomes a tensor of shape (feature_map_height, feature_map_width, n_boxes, 8)\n",
    "            boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
    "            \n",
    "            # Now prepend one dimension to boxes_tensor to account for the batch size and tile it along\n",
    "            # The result will be a 5D tensor of shape (batch_size, feature_map_height, feature_map_width, n_boxes, 8)\n",
    "            \n",
    "            boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
    "            \n",
    "            boxes_tensor=tf.Variable(initial_value=boxes_tensor,dtype=tf.float32)\n",
    "            boxes_tensor=tf.reshape(boxes_tensor,[-1,feature_map_height,feature_map_width,self.n_boxes,8])\n",
    "\n",
    "            return boxes_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(tensor,start_index,conversion,border_pixels='half'):\n",
    "\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1\n",
    "    \n",
    "    ind=start_index\n",
    "    tensor1=np.copy(tensor).astype(np.float)\n",
    "    \n",
    "    if conversion == 'minmax2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] + d # Set h\n",
    "        \n",
    "    elif conversion=='centroids2minmax':\n",
    "        tensor1[...,ind]=(tensor[...,ind] - tensor[...,ind+2] / 2.0) # Set xmin\n",
    "        tensor1[...,ind+1]=(tensor[...,ind] + tensor[...,ind+2] / 2.0) # Set xmax\n",
    "        tensor1[...,ind+2]=(tensor[...,ind+1] - tensor[...,ind+3] / 2.0) # Set ymin\n",
    "        tensor1[...,ind+4]=(tensor[...,ind+1] + tensor[...,ind+3] / 2.0) # Set ymax\n",
    "        \n",
    "    elif conversion == 'corners2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] + d # Set h\n",
    "        \n",
    "    elif conversion == 'centroids2corners':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "        \n",
    "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
    "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
    "        \n",
    "    else:\n",
    "        raise ValueErrorr(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
    "        \n",
    "    return tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.arange(0,38*38*512)\n",
    "Conv_l=data.reshape((-1,38,38,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBox=AnchorBoxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=MyBox.call(Layer=Conv_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(1, 38, 38, 4, 8) dtype=float32>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
