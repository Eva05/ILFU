{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "def ssd_300(features,labels,mode,params={'IMG_SIZE':300}):\n",
    "    \n",
    "    #Reshape IMAGES to 4-D Tensors\n",
    "    \n",
    "    input_layer=tf.reshape(features['x'],[-1,params['IMG_SIZE'],params['IMG_SIZE'],3])\n",
    "    \n",
    "    conv1_1=tf.layers.conv2d(inputs=input_layer,filters=64,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv1_2=tf.layers.conv2d(inputs=conv1_1,filters=64,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    pool1=tf.layers.max_pooling2d(inputs=conv1_2,pool_size=[2,2],strides=2,padding='same')\n",
    "    \n",
    "    #outputs\n",
    "    #(1, 300, 300, 64)\n",
    "    #(1, 300, 300, 64)\n",
    "    #(1, 150, 150, 64)\n",
    "    \n",
    "    \n",
    "    conv2_1=tf.layers.conv2d(inputs=pool1,filters=128,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv2_2=tf.layers.conv2d(inputs=conv2_1,filters=128,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    pool2=tf.layers.max_pooling2d(inputs=conv2_2,pool_size=[2,2],strides=2,padding='same')\n",
    "    \n",
    "    \n",
    "    #outputs\n",
    "    #(1, 150, 150, 128)\n",
    "    #(1, 150, 150, 128)\n",
    "    #(1, 75, 75, 128)\n",
    "    \n",
    "    conv3_1=tf.layers.conv2d(inputs=pool2,filters=256,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv3_2=tf.layers.conv2d(inputs=conv3_1,filters=256,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv3_3=tf.layers.conv2d(inputs=conv3_2,filters=256,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    pool3=tf.layers.max_pooling2d(inputs=conv3_3,pool_size=[2,2],strides=2,padding='same')\n",
    "    \n",
    "    #outputs\n",
    "    #(1, 75, 75, 256)\n",
    "    #(1, 75, 75, 256)\n",
    "    #(1, 75, 75, 256)\n",
    "    #(1, 38, 38, 256)\n",
    "    \n",
    "    conv4_1=tf.layers.conv2d(inputs=pool3,filters=512,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv4_2=tf.layers.conv2d(inputs=conv4_1,filters=512,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv4_3=tf.layers.conv2d(inputs=conv4_2,filters=512,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    pool4=tf.layers.max_pooling2d(inputs=conv4_3,pool_size=[2,2],strides=2,padding='same')\n",
    "\n",
    "    #outputs    \n",
    "    # (1, 38, 38, 512)\n",
    "    # (1, 38, 38, 512)\n",
    "    # (1, 38, 38, 512)\n",
    "    # (1, 19, 19, 512)\n",
    "    \n",
    "    conv5_1=tf.layers.conv2d(inputs=pool4,filters=512,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv5_2=tf.layers.conv2d(inputs=conv5_1,filters=512,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv5_3=tf.layers.conv2d(inputs=conv5_2,filters=512,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    pool5=tf.layers.max_pooling2d(inputs=conv5_3,pool_size=[3,3],strides=1,padding='same')\n",
    "    \n",
    "    #outputs    \n",
    "    # (1, 19, 19, 512)\n",
    "    # (1, 19, 19, 512)\n",
    "    # (1, 19, 19, 512)\n",
    "    # (1, 19, 19, 512)\n",
    "    \n",
    "    fc6=tf.layers.conv2d(inputs=pool5,filters=1024,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    fc7=tf.layers.conv2d(inputs=fc6,filters=1024,kernel_size=[1,1],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv6_1=tf.layers.conv2d(inputs=fc7,filters=256,kernel_size=[1,1],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    #ZERO PADDING##################################################\n",
    "    \n",
    "    conv6_1=tf.pad(conv6_1, [[0, 0], [1, 1], [1, 1], [0, 0]], \"CONSTANT\")\n",
    "    \n",
    "    #ZERO PADDING##################################################\n",
    "    \n",
    "    conv6_2=tf.layers.conv2d(inputs=conv6_1,filters=512,kernel_size=[3,3],strides=(2,2),padding='valid',activation=tf.nn.relu)\n",
    "    \n",
    "    conv7_1=tf.layers.conv2d(inputs=conv6_2,filters=128,kernel_size=[1,1],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    \n",
    "    #ZERO PADDING##################################################\n",
    "    conv7_1=tf.pad(conv7_1, [[0, 0], [1, 1], [1, 1], [0, 0]], \"CONSTANT\")\n",
    "    #ZERO PADDING##################################################\n",
    "    \n",
    "    conv7_2=tf.layers.conv2d(inputs=conv7_1,filters=256,kernel_size=[3,3],strides=(2,2),padding='valid',activation=tf.nn.relu)\n",
    "    \n",
    "    #outputs\n",
    "    # (1, 19, 19, 1024)\n",
    "    # (1, 19, 19, 1024)\n",
    "    # (1, 19, 19, 256)\n",
    "    # (1, 21, 21, 256)\n",
    "    # (1, 10, 10, 512)\n",
    "    # (1, 10, 10, 128)\n",
    "    # (1, 12, 12, 128)\n",
    "    # (1, 5, 5, 256)\n",
    "    \n",
    "    conv8_1=tf.layers.conv2d(inputs=conv7_2,filters=128,kernel_size=[1,1],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv8_2=tf.layers.conv2d(inputs=conv8_1,filters=256,kernel_size=[3,3],padding='valid',strides=1,activation=tf.nn.relu)\n",
    "    \n",
    "    conv9_1=tf.layers.conv2d(inputs=conv8_2,filters=128,kernel_size=[1,1],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv9_2=tf.layers.conv2d(inputs=conv9_1,filters=256,kernel_size=[3,3],padding='valid',strides=1,activation=tf.nn.relu)\n",
    "    \n",
    "    #outputs\n",
    "    # (1, 5, 5, 128)\n",
    "    # (1, 3, 3, 256)\n",
    "    # (1, 3, 3, 128)\n",
    "    # (1, 1, 1, 256)    \n",
    "    \n",
    "    # Feed conv4_3 into the L2 normalization layer\n",
    "    # conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n",
    "    \n",
    "    conv4_3_norm=tf.nn.l2_normalize(conv4_3)\n",
    "    \n",
    "    #outputs\n",
    "    # (1, 38, 38, 512) \n",
    "    \n",
    "    \n",
    "    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n",
    "    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)\n",
    "    \n",
    "    #CONFIDENCE LEVEL\n",
    "    conv4_3_norm_mbox_conf=tf.layers.conv2d(inputs=conv4_3_norm,filters=n_boxes[0]*n_classes,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    fc7_mbox_conf=tf.layers.conv2d(inputs=fc7,filters=n_boxes[1]*n_classes,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv6_2_mbox_conf=tf.layers.conv2d(inputs=conv6_2,filters=n_boxes[2]*n_classes,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv7_2_mbox_conf=tf.layers.conv2d(inputs=conv7_2,filters=n_boxes[3]*n_classes,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv8_2_mbox_conf=tf.layers.conv2d(inputs=conv8_2,filters=n_boxes[4]*n_classes,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv9_2_mbox_conf=tf.layers.conv2d(inputs=conv9_2,filters=n_boxes[5]*n_classes,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    #outputs\n",
    "    # (1, 38, 38, 80)\n",
    "    # (1, 19, 19, 80)\n",
    "    # (1, 10, 10, 80)\n",
    "    # (1, 5, 5, 120)\n",
    "    # (1, 3, 3, 120)\n",
    "    # (1, 1, 1, 120)\n",
    "    \n",
    "    \n",
    "    # We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n",
    "    # Output shape of the localization layers: `(batch, height, width, n_boxes * 4)\n",
    "    \n",
    "    #LOCATION LEVEL\n",
    "    \n",
    "    conv4_3_norm_mbox_loc=tf.layers.conv2d(inputs=conv4_3_norm,filters=n_boxes[0]*4,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    fc7_mbox_loc=tf.layers.conv2d(inputs=fc7,filters=n_boxes[1]*4,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv6_2_mbox_loc=tf.layers.conv2d(inputs=conv6_2,filters=n_boxes[2]*4,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv7_2_mbox_loc=tf.layers.conv2d(inputs=conv7_2,filters=n_boxes[3]*4,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv8_2_mbox_loc=tf.layers.conv2d(inputs=conv8_2,filters=n_boxes[4]*4,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    conv9_2_mbox_loc=tf.layers.conv2d(inputs=conv9_2,filters=n_boxes[5]*4,kernel_size=[3,3],padding='same',activation=tf.nn.relu)\n",
    "    \n",
    "    #outputs\n",
    "    # (1, 38, 38, 16)\n",
    "    # (1, 19, 19, 16)\n",
    "    # (1, 10, 10, 16)\n",
    "    # (1, 5, 5, 24)\n",
    "    # (1, 3, 3, 24)\n",
    "    # (1, 1, 1, 24)\n",
    "    \n",
    "    #GENERATE ANCHORS\n",
    "    \n",
    "    #GENERATE ANCHORS \n",
    "    \n",
    "    \n",
    "    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n",
    "    # We want the classes isolated in the last axis to perform softmax on them\n",
    "    \n",
    "    conv4_3_norm_mbox_conf_reshape=tf.reshape(conv4_3_norm_mbox_conf,[-1,n_classes])\n",
    "    \n",
    "    fc7_mbox_conf_reshape=tf.reshape(fc7_mbox_conf,[-1,n_classes])\n",
    "    \n",
    "    conv6_2_mbox_conf_reshape=tf.reshape(conv6_2_mbox_conf,[-1,n_classes])\n",
    "    \n",
    "    conv7_2_mbox_conf_reshape=tf.reshape(conv7_2_mbox_conf,[-1,n_classes])\n",
    "    \n",
    "    conv8_2_mbox_conf_reshape=tf.reshape(conv8_2_mbox_conf,[-1,n_classes])\n",
    "    \n",
    "    conv9_2_mbox_conf_reshape=tf.reshape(conv9_2_mbox_conf,[-1,n_classes])\n",
    "    \n",
    "    #outputs\n",
    "    # (5776, 20)\n",
    "    # (1444, 20)\n",
    "    # (400, 20)\n",
    "    # (150, 20)\n",
    "    # (54, 20)\n",
    "    # (6, 20)\n",
    "    \n",
    "    # Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)\n",
    "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
    "    \n",
    "    conv4_3_norm_mbox_loc_reshape=tf.reshape(conv4_3_norm_mbox_loc,[-1,4])\n",
    "    \n",
    "    fc7_mbox_loc_reshape=tf.reshape(fc7_mbox_loc,[-1,4])\n",
    "    \n",
    "    conv6_2_mbox_loc_reshape=tf.reshape(conv6_2_mbox_loc,[-1,4])\n",
    "    \n",
    "    conv7_2_mbox_loc_reshape=tf.reshape(conv7_2_mbox_loc,[-1,4])\n",
    "    \n",
    "    conv8_2_mbox_loc_reshape=tf.reshape(conv8_2_mbox_loc,[-1,4])\n",
    "    \n",
    "    conv9_2_mbox_loc_reshape=tf.reshape(conv9_2_mbox_loc,[-1,4])\n",
    "    \n",
    "    #outputs\n",
    "    # (5776, 4)\n",
    "    # (1444, 4)\n",
    "    # (400, 4)\n",
    "    # (150, 4)\n",
    "    # (54, 4)\n",
    "    # (6, 4)\n",
    "    \n",
    "    #RESHAPE ANCHORS###########################\n",
    "    \n",
    "    #RESHAPE ANCHORS###########################\n",
    "    \n",
    "    \n",
    "    ### Concatenate the predictions from the different layers\n",
    "\n",
    "    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n",
    "    # so we want to concatenate along axis 1, the number of boxes per layer\n",
    "    # Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n",
    "    \n",
    "    mbox_conf=tf.concat(values=[conv4_3_norm_mbox_conf_reshape,\n",
    "                           fc7_mbox_conf_reshape,\n",
    "                           conv6_2_mbox_conf_reshape,\n",
    "                           conv7_2_mbox_conf_reshape,\n",
    "                           conv8_2_mbox_conf_reshape,\n",
    "                           conv9_2_mbox_conf_reshape],axis=0)\n",
    "    \n",
    "    # Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n",
    "    \n",
    "    mbox_conf=tf.concat(values=[conv4_3_norm_mbox_loc_reshape,\n",
    "                                fc7_mbox_loc_reshape,\n",
    "                                conv6_2_mbox_loc_reshape,\n",
    "                                conv7_2_mbox_loc_reshape,\n",
    "                                conv8_2_mbox_loc_reshape,\n",
    "                                conv9_2_mbox_loc_reshape],axis=0)\n",
    "    \n",
    "    # Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n",
    "    \n",
    "    #CONCATENATE PRIORBOX#####################################\n",
    "    \n",
    "    \n",
    "    #CONCATENATE PRIORBOX#####################################\n",
    "    \n",
    "    # The box coordinate predictions will go into the loss function just the way they are,\n",
    "    # but for the class predictions, we'll apply a softmax activation layer first\n",
    "    \n",
    "    mbox_conf_softmax=tf.nn.softmax(logits=mbox_conf,axis=0)    \n",
    "    \n",
    "    # Concatenate the class and box predictions and the anchors to one large predictions vector\n",
    "    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
    "    \n",
    "    ####PRIORBOX IS NO DEFINED YET!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    predictions=tf.concat(values=[mbox_conf_softmax, mbox_loc, mbox_priorbox],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
